

Welcome to Day 1 of the DSA Coding Challenge:

Today's Goals (Arrays) :

-Learn Big O Analysis to find Time and Space complexity

-Array Data Structure Crash Course

Question 1: Sorted Squared Array - You are given an array of Integers in which each subsequent value is not less than the previous value.
 Write a function that takes this array as an input and returns a new array with the squares of each number sorted in ascending order.

Question 2: Monotonic Array - An array is monotonic if it is either monotone increasing or monotone decreasing. 
An array is monotone increasing if all its elements from left to right are non-decreasing.
An array is monotone decreasing if all  its elements from left to right are non-increasing. Given an integer array return true if the given array is monotonic, or false otherwise.

All the best ! 

------------------------------------------------------


What are data structures ?
    ->Collection of data values 
    ->relationships among them 
    ->functions or operations that can be applied on 
example arr=[1,4,6,2]
    ->here we agree they ara collection of data values
    ->we know relationship among them like their indexes
    ->operation can be performed like push ,pop etc in array 


why do you need to learn data structures  to ace coding interviews?
    ->we code here with better time complexity and space complexity ,and we know which one data structure to use to make it efficient code 


-------------------------------------------------------------

Big 0 
-------

What is the need for complexity analysis ?
example: 

the given input is 5               we need ouput to be 5 factorial like 4-1+3-1+2-1 etc 

here two approach can be derived 
    -> a loop 
    ->n*(n-1)
     ----------
        2

Both also gives answer 

    -Which approach is better ?
        ~ which approach have best less number of operations as the input grows is known as best 
    -why care about identifying which is better?
        ~ we know facebook and instagram have billions of user data and also users this will have a huge performace issue ,so we need to take which solution far better to handle the data 
    -what does better mean?
        ~ faster -> time complexity 
        ~ less memory -> space complexity


Time complexity 
--------------
    don'ts
        -> we do not measure the time here like how many seconds it took to run the code because we can see even if we run the same code twice there is difference in millisecons
        ->different machines have different time like hardware configurations 

    do's
        -> we count the number of operations performed on the code ,by this way even if we run on any hardware the number of operations matters 
        -> as input grows does the number of operation also grow in regards to the input then the algorithm is not good in many cases 
        -> if the input grows then in what proportion does the number of operatioon grows ,to find this we have Asymptotic analysis(it is going to be expressed in big 0 notations)
        -> Time complexity in simple terms how (runtime-> means number of simple operations) grows as input size grows
    
Asymptotic Analysis 
--------------------

        function of n means 
            ~  x is the input then f(x) is the output 
            f(x)=x^2+1

    -> eg:f(n)=n+3 here why they are negating constant is example if we take n as billion then the constant is of no use there we need just the steep on graph
            not the precision we need to know whether the graph on input increase does the operation also increase with it 

            here f(n)=n that is big o(n) that is linear 

    -> asymtotic analysis is nothing but determinig the big o from the function


Big O 
-------
    ->operation is bounded by a multiple of N 
    ->we are interested in trends not the details 
    -> Worst case / upper bound 

    ->as the input grows we calculate how much time and space it took for that particular algorithm  


Commmon complexity
-------------------
 linear graph means 

        |              
        |                 -
        |              -
        |           -
        |        -
        |     -
        |  -
        |-_________________________

    ->refer the image for complexity
    

Space complexity
------------------
    *Auxiliary space = extra working memory used by the algorithm, not counting the input itself.*
    -> here when we run the code ,we wont consider the space which the input is stored ,we consider how much auxilary space required to run  the algorithm
    -> primitive types are constant space 

    -> different parameter of input cannot be dropped example :O(n^3+m) cannot drop because n maybe 1 but m is billion then this like senario may occurs ,so we cannot drop